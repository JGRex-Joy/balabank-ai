# ğŸ¦ Balabank AI API

**Balabank AI API** is an intelligent question-answering system built with FastAPI that provides personalized responses based on financial literature. The API uses RAG (Retrieval-Augmented Generation) architecture with Google's Gemini models to deliver context-aware answers tailored for different audiences.

---

## âœ¨ Features

- ğŸ¤– **Role-based responses**: Tailored answers for children and parents
- ğŸ” **Semantic search**: FAISS-powered vector similarity search
- ğŸ§  **LLM integration**: Powered by Google Gemini 2.5 Flash
- ğŸ“š **RAG architecture**: Retrieval-Augmented Generation for accurate, context-aware responses
- ğŸš€ **Fast and efficient**: Built with FastAPI for high performance
- ğŸ“Š **Embeddings**: Custom vector embeddings using Gemini Embedding model

---

## ğŸ›  Tech Stack

- **Framework**: FastAPI
- **LLM**: Google Gemini 2.5 Flash
- **Embeddings**: Google Gemini Embedding (768 dimensions)
- **Vector Search**: FAISS
- **Language**: Python 3.x
- **Data Processing**: NumPy, JSON

---

## ğŸ“ Project Structure

```
balabank-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ ask.py          # Ask endpoints (children/parent)
â”‚   â”‚   â””â”€â”€ health.py       # Health check endpoint
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM integration
â”‚   â”‚   â”œâ”€â”€ search.py       # Vector search logic
â”‚   â”‚   â””â”€â”€ server_embedder.py  # Embedding service
â”‚   â””â”€â”€ main.py             # FastAPI application
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chunks.json         # Text chunks from financial books
â”‚   â””â”€â”€ index.faiss         # FAISS vector index
â””â”€â”€ README.md
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- Google AI Studio API key

### Steps

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/balabank-ai-api.git
cd balabank-ai-api
```

2. **Create virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install fastapi uvicorn google-generativeai faiss-cpu numpy pydantic
```

4. **Prepare data files**

Ensure you have the following files in the `data/` directory:
- `chunks.json` - JSON array of text chunks with metadata
- `index.faiss` - Pre-built FAISS index

---

## âš™ï¸ Configuration

### API Keys

Update the Google API key in the following files:

**`app/services/llm.py`**
```python
client = genai.Client(api_key="YOUR_GOOGLE_API_KEY_HERE")
```

**`app/services/server_embedder.py`**
```python
client = genai.Client(api_key="YOUR_GOOGLE_API_KEY_HERE")
```

> **Note**: For production, use environment variables instead of hardcoding API keys.

### Environment Variables (Recommended)

```bash
export GOOGLE_API_KEY="your_api_key_here"
```

Then update the code:
```python
import os
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
```

---

## ğŸ¯ Usage

### Running the API

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### Interactive API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

---

## ğŸ“¡ API Endpoints

### 1. Health Check

```http
GET /health/
```

**Response:**
```json
{
  "status": "ok"
}
```

---

### 2. Ask for Children

```http
POST /ask/children
```

**Request Body:**
```json
{
  "prompt": "What is saving money?"
}
```

**Response:**
```json
{
  "llm_answer": "Saving money is like putting coins in a piggy bank! It means you keep some of your money safe instead of spending it all right away..."
}
```

---

### 3. Ask for Parents

```http
POST /ask/parent
```

**Request Body:**
```json
{
  "prompt": "What investment strategies are recommended?"
}
```

**Response:**
```json
{
  "llm_answer": "Based on the financial literature, recommended investment strategies include diversification, long-term planning, and risk assessment..."
}
```

---

## ğŸ”„ How It Works

### RAG Pipeline

1. **Query Reception**: User sends a question via API
2. **Embedding Generation**: Query is converted to a 768-dimensional vector
3. **Semantic Search**: FAISS finds top-K most similar text chunks
4. **Context Building**: Retrieved chunks are combined into context
5. **LLM Generation**: Gemini generates a role-appropriate answer
6. **Response Delivery**: Answer is returned to the user

### Architecture Diagram

```
User Query â†’ Embedder â†’ FAISS Search â†’ Context Retrieval
                                              â†“
                                         LLM (Gemini)
                                              â†“
                                    Role-based Answer â†’ User
```

---

## ğŸ’¡ Examples

### Using cURL

**Children endpoint:**
```bash
curl -X POST "http://localhost:8000/ask/children" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Why should I save money?"}'
```

**Parent endpoint:**
```bash
curl -X POST "http://localhost:8000/ask/parent" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "How to teach children about financial literacy?"}'
```

### Using Python

```python
import requests

url = "http://localhost:8000/ask/children"
data = {"prompt": "What is a bank?"}

response = requests.post(url, json=data)
print(response.json()["llm_answer"])
```

---

## ğŸ“ TODO

- [ ] Add authentication and authorization
- [ ] Implement rate limiting
- [ ] Add logging and monitoring
- [ ] Create unit and integration tests
- [ ] Add Docker support
- [ ] Implement caching for frequent queries
- [ ] Add more role types (teens, educators, etc.)
- [ ] Multi-language support

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Amir Omurkulov**

JGRex-Joy - Junior AI Engineer

Built with â¤ï¸ as a legal-tech & LLM/AI engineering project.

---

## ğŸ™ Acknowledgments

- Google Gemini AI for powerful LLM capabilities
- FAISS for efficient vector search
- FastAPI for the excellent web framework

---

